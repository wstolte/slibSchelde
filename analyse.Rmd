---
title: "Licht-sediment relatie in de Schelde"
output: html_notebook
---

```{r}
require(tidyverse)
require(skimr)
require(leaflet)
```

## Data extraction

The Scheldemonitor was queried for parameters related to light extinction and suspended sediment.



```{r leesDataUitScheldemonitor, cache=T}
url <- "http://geo.vliz.be/geoserver/wfs/ows?service=WFS&version=1.1.0&request=GetFeature&typeName=Dataportal%3Aabiotic_observations&resultType=results&viewParams=where%3Aobs.context+%26%26+ARRAY%5B1%5D+AND+standardparameterid+IN+%283240%5C%2C461%5C%2C1293%5C%2C495%5C%2C3289%5C%2C1223%29%3Bcontext%3A0001&propertyName=stationname%2Clongitude%2Clatitude%2Cdatetime%2Cdepth%2Cparametername%2Cvaluesign%2Cvalue%2Cdataprovider%2Cdatasettitle&outputFormat=csv"

df <- read_csv(url, na = "-")
```

## Parameters

```{r}
df %>% distinct(parametername, datasettitle)
```

### Spatial extent per dataset

```{r}

pal = colorFactor(topo.colors(10), domain = as.factor(df$datasettitle))
df %>% distinct(latitude, longitude, datasettitle) %>%
  mutate(datasettitle = as.factor(datasettitle)) %>%
  filter(latitude>0) %>%
  leaflet() %>%
  addTiles() %>%
  addCircleMarkers(
    stroke = F, fillOpacity = 1, fillColor =  ~pal(datasettitle),
    label = ~datasettitle) #%>%
  # addLegend("bottomright", pal = pal, values = ~datasettitle,
    # title = "dataset",
    # opacity = 1)
```


## Find duplicates

Data may be reported twice to Scheldemonitor by different organizations. 

```{r}
# duplicate rows?
df %>% 
  drop_na(value) %>%
  duplicated() %>% which() %>% length()
# no


# duplicate measurements?
df %>% 
  drop_na(value) %>%
  select(latitude, longitude, depth, datetime, parametername, stationname, value)  %>%
  duplicated() %>% which() %>% length()
# yes. 1

```

```{r remove duplicate measurements}
duplicates <- df %>% 
  select(latitude, longitude, depth, datetime, parametername, stationname, value)  %>%
  duplicated() %>% which()
df_clean <- df[-duplicates,]

# There may be multiple values at same location and time
df_clean %>% 
  drop_na(value) %>%
  select(latitude, longitude, depth, datetime, parametername, stationname)  %>%
  duplicated() %>% which() %>% length()
# yes, 54

# See if we can average them
dupliSamples <- df_clean %>% 
  drop_na(value) %>%
  select(latitude, longitude, depth, datetime, parametername, stationname)  %>%
  duplicated() %>% which()

df_clean %>%
  group_by(latitude, longitude, depth, datetime, parametername, stationname) %>%
  summarize(n = n(), mean = mean(value, na.rm = T), sd = sd(value, na.rm = T)) %>%
  arrange(-n) %>% head(54)

# Some sd values are very high compared to mean. Still, averaging is necessary to work with the data further

# redefine df_clean without any duplicate measurements

df_clean <- df[-duplicates,] %>%
  select(latitude, longitude, depth, datetime, parametername, stationname, value) %>%
  group_by(latitude, longitude, depth, datetime, parametername, stationname) %>%
  summarize(mean = mean(value, na.rm = T))

```


There are some samples with very high sd as compared to the mean (n = 2). 

Perhaps take mean anyway? Other solutions?



## Find matching parameters

Perhaps first aggregate parameters that can be aggregated?

```{r aggregate parameters}

df %>% distinct(parametername, datasettitle)

```




Find parameters that were measured at the same time and location




```{r matchParameters}
# make cross-table
df_clean %>% 
  select(latitude, longitude, depth, datetime, parametername, stationname, value) %>%
  group_by(latitude, longitude, depth, datetime, parametername, stationname) %>%
  summarize(mean = mean(value, na.rm = T)) %>%
  spread(key = parametername, value = mean) %>%
  filter()
  

```


